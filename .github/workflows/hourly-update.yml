name: Hourly Data Update

on:
  schedule:
    # Run every hour at minute 5 (avoid exactly on the hour when many jobs run)
    - cron: '5 * * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI

env:
  PYTHONUNBUFFERED: '1'  # Real-time output for all Python scripts

jobs:
  update-data:
    runs-on: ubuntu-latest

    permissions:
      contents: write  # Needed to push commits

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Restore database from cache
        uses: actions/cache@v4
        id: cache-db
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}
          restore-keys: |
            analytics-db-

      - name: Create data directory
        run: mkdir -p data

      - name: Check if database exists (from cache)
        id: check-db
        run: |
          if [ -f "data/analytics.duckdb" ]; then
            echo "exists=true" >> $GITHUB_OUTPUT
            SIZE=$(du -h data/analytics.duckdb | cut -f1)
            echo "‚úÖ Database found in cache ($SIZE)"
          else
            echo "exists=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Database not found in cache"
          fi

      - name: Download database from release (if not cached)
        if: steps.check-db.outputs.exists == 'false'
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üì• Downloading database from release..."
          if gh release download database --pattern "analytics.duckdb" --dir data/ 2>/dev/null; then
            SIZE=$(du -h data/analytics.duckdb | cut -f1)
            echo "‚úÖ Downloaded database from release ($SIZE)"
          else
            echo "‚ùå No database release found!"
            echo "To bootstrap: create a release tagged 'database' with analytics.duckdb attached"
            exit 1
          fi

      # YAML HEREDOC SYNTAX FOR INLINE PYTHON
      # DO NOT use `python -c "..."` for multiline code - YAML parsing breaks.
      # See GOTCHAS.md for context.
      - name: Verify database and show status
        id: verify-db
        run: |
          cd scripts
          python3 << 'PYTHON_SCRIPT'
          import sys
          from db import get_connection
          import json
          from datetime import datetime, timezone

          print('=' * 60)
          print('DATABASE STATUS BEFORE UPDATE')
          print('=' * 60)
          print(f'Current time: {datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")}')
          print()

          conn = get_connection()

          with open('assets.json') as f:
              assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

          all_ok = True
          for asset in assets:
              asset_id = asset['id']
              tweet_result = conn.execute('''
                  SELECT MAX(timestamp) as latest, COUNT(*) as total
                  FROM tweets WHERE asset_id = ?
              ''', [asset_id]).fetchone()
              price_result = conn.execute('''
                  SELECT MAX(timestamp) as latest, COUNT(*) as total
                  FROM prices WHERE asset_id = ? AND timeframe = '1h'
              ''', [asset_id]).fetchone()
              tweet_latest = tweet_result[0] if tweet_result[0] else 'none'
              price_latest = price_result[0] if price_result[0] else 'none'
              print(f'{asset_id.upper():10} | Tweets: {tweet_result[1]:4} (latest: {tweet_latest})')
              print(f'           | Prices: {price_result[1]:5} 1h candles (latest: {price_latest})')
              if price_result[1] == 0:
                  print(f'           | WARNING: No price data!')
                  all_ok = False
              print()

          conn.close()
          if not all_ok:
              print('Some assets have missing data - fetch may take longer')
          else:
              print('All assets have data')
          PYTHON_SCRIPT
          echo "db_verified=true" >> $GITHUB_OUTPUT

      - name: Fetch new tweets
        env:
          X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        run: |
          echo "üê¶ Fetching new tweets..."
          cd scripts
          python fetch_tweets.py 2>&1 || echo "‚ö†Ô∏è Tweet fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 10

      - name: Fetch new prices (15m, 1h, 1d)
        env:
          BIRDEYE_API_KEY: ${{ secrets.BIRDEYE_API_KEY }}
        run: |
          echo "Fetching new prices (incremental - only missing data)..."
          cd scripts
          # Default mode: incremental (checks DB for last timestamp, only fetches new data)
          python fetch_prices.py -t 15m -t 1h -t 1d 2>&1 || echo "Price fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 5

      - name: Compute statistics
        run: |
          echo "üìà Computing statistics..."
          cd scripts
          python compute_stats.py 2>&1 || echo "‚ö†Ô∏è Stats computation had errors"
        continue-on-error: true
        timeout-minutes: 5

      - name: Export static data
        id: export
        run: |
          echo "üì¶ Exporting static JSON files..."
          cd scripts
          python export_static.py 2>&1

          # Verify export produced files
          if [ -d "../web/public/static/pump" ]; then
            echo "‚úÖ Export completed"
            echo "export_ok=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Export failed - no output files!"
            echo "export_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        timeout-minutes: 5

      - name: Validate exported data
        if: steps.export.outputs.export_ok == 'true'
        run: |
          echo "Validating exported data..."
          cd scripts
          python3 << 'PYTHON_SCRIPT'
          import json
          from pathlib import Path

          static_dir = Path('../web/public/static')
          errors = []

          with open('assets.json') as f:
              assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

          print()
          for asset in assets:
              asset_id = asset['id']
              asset_dir = static_dir / asset_id

              required = ['tweet_events.json', 'prices_1h.json', 'prices_1d.json', 'stats.json']
              missing = [f for f in required if not (asset_dir / f).exists()]

              if missing:
                  errors.append(f'{asset_id}: missing {missing}')
                  print(f'MISSING {asset_id.upper()}: {missing}')
                  continue

              with open(asset_dir / 'tweet_events.json') as f:
                  tweet_count = json.load(f).get('count', 0)

              with open(asset_dir / 'prices_1h.json') as f:
                  price_count = json.load(f).get('count', 0)

              if tweet_count == 0:
                  errors.append(f'{asset_id}: no tweets')
                  print(f'WARN {asset_id.upper()}: {tweet_count} tweets, {price_count} prices')
              elif price_count == 0:
                  errors.append(f'{asset_id}: no prices')
                  print(f'ERROR {asset_id.upper()}: {tweet_count} tweets, {price_count} prices')
              else:
                  print(f'OK {asset_id.upper()}: {tweet_count} tweets, {price_count} prices')

          print()
          if errors:
              print(f'Validation warnings: {len(errors)}')
              for e in errors:
                  print(f'   - {e}')
          else:
              print('All assets validated successfully')
          PYTHON_SCRIPT

      - name: Generate timestamp
        run: |
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          echo "{\"timestamp\": \"$TIMESTAMP\", \"source\": \"github-actions\"}" > web/public/static/last_updated.json
          echo "‚è∞ Timestamp: $TIMESTAMP"

      - name: Check for changes
        id: check-changes
        run: |
          git add -A
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "üìù Changes detected:"
            git diff --cached --stat | head -20
          fi

      - name: Commit and push changes
        if: steps.check-changes.outputs.changed == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
          git commit -m "chore: hourly data update $TIMESTAMP"
          git push
          echo "‚úÖ Changes pushed to repository"

      - name: Save database to cache
        uses: actions/cache/save@v4
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}

      - name: Upload database to release (backup)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üíæ Backing up database to release..."
          gh release upload database data/analytics.duckdb --clobber 2>/dev/null || \
            gh release create database data/analytics.duckdb --title "Database Backup" --notes "Auto-updated by hourly workflow"
          echo "‚úÖ Database backed up"

      - name: Summary
        run: |
          echo ""
          echo "========================================"
          echo "           WORKFLOW COMPLETE"
          echo "========================================"
          if [ "${{ steps.check-changes.outputs.changed }}" == "true" ]; then
            echo "‚úÖ Data updated and pushed"
            echo "üöÄ Vercel will auto-deploy shortly"
          else
            echo "‚ÑπÔ∏è  No new data to update"
          fi
          echo "========================================"
