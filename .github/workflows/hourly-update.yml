name: Hourly Data Update

on:
  schedule:
    # Run every hour at minute 5 (avoid exactly on the hour when many jobs run)
    - cron: '5 * * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub UI

env:
  PYTHONUNBUFFERED: '1'  # Real-time output for all Python scripts

jobs:
  update-data:
    # Use self-hosted runner to bypass Cloudflare blocking GitHub's datacenter IPs
    # The runner is on satoshi's Mac with a clean residential IP
    runs-on: self-hosted

    permissions:
      contents: write  # Needed to push commits

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Set up Python
        run: |
          # Use system Python (works on both GitHub-hosted and self-hosted runners)
          if command -v python3 &> /dev/null; then
            echo "Using system Python:"
            python3 --version
          else
            echo "ERROR: Python3 not found"
            exit 1
          fi

      - name: Install dependencies
        run: |
          # Create venv to isolate dependencies
          python3 -m venv .venv || true
          source .venv/bin/activate 2>/dev/null || true
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirements.txt

      - name: Restore database from cache
        uses: actions/cache@v4
        id: cache-db
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}
          restore-keys: |
            analytics-db-

      - name: Create data directory
        run: mkdir -p data

      - name: Validate database (prefer larger of cache vs release)
        id: check-db
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          CACHE_SIZE=0
          RELEASE_SIZE=0
          
          # Check cache size
          if [ -f "data/analytics.duckdb" ]; then
            CACHE_SIZE=$(stat -c%s "data/analytics.duckdb" 2>/dev/null || stat -f%z "data/analytics.duckdb" 2>/dev/null || echo 0)
            echo "üì¶ Cache database: $(numfmt --to=iec $CACHE_SIZE 2>/dev/null || echo ${CACHE_SIZE} bytes)"
          else
            echo "üì¶ Cache database: not found"
          fi
          
          # Check release size (via API to avoid downloading)
          RELEASE_INFO=$(gh api repos/${{ github.repository }}/releases/tags/database --jq '.assets[] | select(.name=="analytics.duckdb") | .size' 2>/dev/null || echo 0)
          RELEASE_SIZE=${RELEASE_INFO:-0}
          echo "‚òÅÔ∏è  Release database: $(numfmt --to=iec $RELEASE_SIZE 2>/dev/null || echo ${RELEASE_SIZE} bytes)"
          
          # Decision: use larger database
          if [ "$RELEASE_SIZE" -gt "$CACHE_SIZE" ]; then
            echo "‚¨áÔ∏è  Release is larger - downloading fresh copy..."
            rm -f data/analytics.duckdb
            if gh release download database --pattern "analytics.duckdb" --dir data/ 2>/dev/null; then
              SIZE=$(du -h data/analytics.duckdb | cut -f1)
              echo "‚úÖ Downloaded database from release ($SIZE)"
            else
              echo "‚ùå Download failed!"
              exit 1
            fi
          elif [ "$CACHE_SIZE" -gt 0 ]; then
            echo "‚úÖ Using cached database (same size or larger)"
          else
            echo "‚¨áÔ∏è  No cache - downloading from release..."
            if gh release download database --pattern "analytics.duckdb" --dir data/ 2>/dev/null; then
              SIZE=$(du -h data/analytics.duckdb | cut -f1)
              echo "‚úÖ Downloaded database from release ($SIZE)"
            else
              echo "‚ùå No database found in cache or release!"
              exit 1
            fi
          fi
          
          echo "db_validated=true" >> $GITHUB_OUTPUT

      # YAML HEREDOC SYNTAX FOR INLINE PYTHON
      # DO NOT use `python -c "..."` for multiline code - YAML parsing breaks.
      # See GOTCHAS.md for context.
      - name: Verify database and show status
        id: verify-db
        run: |
          source .venv/bin/activate 2>/dev/null || true
          cd scripts
          python3 << 'PYTHON_SCRIPT'
          import sys
          from db import get_connection
          import json
          from datetime import datetime, timezone

          print('=' * 60)
          print('DATABASE STATUS BEFORE UPDATE')
          print('=' * 60)
          print(f'Current time: {datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")}')
          print()

          conn = get_connection()

          with open('assets.json') as f:
              assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

          all_ok = True
          for asset in assets:
              asset_id = asset['id']
              tweet_result = conn.execute('''
                  SELECT MAX(timestamp) as latest, COUNT(*) as total
                  FROM tweets WHERE asset_id = ?
              ''', [asset_id]).fetchone()
              price_result = conn.execute('''
                  SELECT MAX(timestamp) as latest, COUNT(*) as total
                  FROM prices WHERE asset_id = ? AND timeframe = '1h'
              ''', [asset_id]).fetchone()
              tweet_latest = tweet_result[0] if tweet_result[0] else 'none'
              price_latest = price_result[0] if price_result[0] else 'none'
              print(f'{asset_id.upper():10} | Tweets: {tweet_result[1]:4} (latest: {tweet_latest})')
              print(f'           | Prices: {price_result[1]:5} 1h candles (latest: {price_latest})')
              if price_result[1] == 0:
                  print(f'           | WARNING: No price data!')
                  all_ok = False
              print()

          conn.close()
          if not all_ok:
              print('Some assets have missing data - fetch may take longer')
          else:
              print('All assets have data')
          PYTHON_SCRIPT
          echo "db_verified=true" >> $GITHUB_OUTPUT

      - name: Fetch new tweets
        env:
          X_BEARER_TOKEN: ${{ secrets.X_BEARER_TOKEN }}
        run: |
          echo "üê¶ Fetching new tweets..."
          cd scripts
          source ../.venv/bin/activate 2>/dev/null || true
          python3 fetch_tweets.py 2>&1 || echo "‚ö†Ô∏è Tweet fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 10

      - name: Fetch new prices (15m, 1h, 1d)
        env:
          BIRDEYE_API_KEY: ${{ secrets.BIRDEYE_API_KEY }}
          COINGECKO_API_KEY: ${{ secrets.COINGECKO_API_KEY }}
        run: |
          echo "Fetching new prices (incremental - only missing data)..."
          cd scripts
          # Default mode: incremental (checks DB for last timestamp, only fetches new data)
          source ../.venv/bin/activate 2>/dev/null || true
          python3 fetch_prices.py -t 15m -t 1h -t 1d 2>&1 || echo "Price fetch had errors (continuing anyway)"
        continue-on-error: true
        timeout-minutes: 10

      - name: Cleanup any orphaned processes
        if: always()
        run: |
          # Kill any Python processes that might hold DuckDB locks
          pkill -9 -f "python.*fetch_prices" 2>/dev/null || true
          pkill -9 -f "python.*fetch_tweets" 2>/dev/null || true
          # Remove any stale lock files
          rm -f data/analytics.duckdb.wal 2>/dev/null || true
          rm -f data/analytics.duckdb.tmp 2>/dev/null || true
          echo "‚úÖ Cleanup complete"

      - name: Compute statistics
        run: |
          echo "üìà Computing statistics..."
          cd scripts
          source ../.venv/bin/activate 2>/dev/null || true
          python3 compute_stats.py 2>&1 || echo "‚ö†Ô∏è Stats computation had errors"
        continue-on-error: true
        timeout-minutes: 5

      - name: Snapshot current data counts (pre-export)
        id: pre-snapshot
        run: |
          echo "üì∏ Capturing pre-export data counts..."
          source .venv/bin/activate 2>/dev/null || true
          cd scripts
          python3 << 'PYTHON_SCRIPT'
          import json
          from pathlib import Path

          static_dir = Path('../web/public/static')
          snapshot = {}

          with open('assets.json') as f:
              assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

          for asset in assets:
              asset_id = asset['id']
              asset_dir = static_dir / asset_id
              counts = {'1h': 0, '1d': 0, '15m': 0, 'tweets': 0}

              for tf in ['1h', '1d', '15m']:
                  f_path = asset_dir / f'prices_{tf}.json'
                  if f_path.exists():
                      with open(f_path) as f:
                          counts[tf] = json.load(f).get('count', 0)

              tweets_path = asset_dir / 'tweet_events.json'
              if tweets_path.exists():
                  with open(tweets_path) as f:
                      counts['tweets'] = json.load(f).get('count', 0)

              snapshot[asset_id] = counts
              print(f'{asset_id}: 1h={counts["1h"]}, 1d={counts["1d"]}, 15m={counts["15m"]}, tweets={counts["tweets"]}')

          # Save snapshot for comparison
          with open('/tmp/pre_export_snapshot.json', 'w') as f:
              json.dump(snapshot, f)
          PYTHON_SCRIPT

      - name: Cleanup before export
        run: |
          echo "üßπ Killing any lingering Python processes before export..."
          pkill -9 -f "python.*compute_stats" 2>/dev/null || true
          pkill -9 -f "python.*fetch" 2>/dev/null || true
          # Remove stale DuckDB lock files
          rm -f data/analytics.duckdb.wal 2>/dev/null || true
          rm -f data/analytics.duckdb.tmp 2>/dev/null || true
          sleep 2
          echo "‚úÖ Cleanup complete"

      - name: Export static data
        id: export
        run: |
          echo "üì¶ Exporting static JSON files..."
          cd scripts
          source ../.venv/bin/activate 2>/dev/null || true
          python3 export_static.py --no-validate 2>&1

          # Verify export produced files
          if [ -d "../web/public/static/pump" ]; then
            echo "‚úÖ Export completed"
            echo "export_ok=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Export failed - no output files!"
            echo "export_ok=false" >> $GITHUB_OUTPUT
            exit 1
          fi
        timeout-minutes: 5

      - name: Validate exported data (prevent truncation)
        id: validate
        if: steps.export.outputs.export_ok == 'true'
        run: |
          echo "üîç Validating exported data (checking for truncation)..."
          source .venv/bin/activate 2>/dev/null || true
          cd scripts
          python3 << 'PYTHON_SCRIPT'
          import json
          import sys
          from pathlib import Path

          static_dir = Path('../web/public/static')
          errors = []
          truncation_errors = []

          # Load pre-export snapshot
          try:
              with open('/tmp/pre_export_snapshot.json') as f:
                  pre_snapshot = json.load(f)
          except:
              pre_snapshot = {}

          with open('assets.json') as f:
              assets = [a for a in json.load(f)['assets'] if a.get('enabled', True)]

          print()
          print('=' * 60)
          print('DATA TRUNCATION CHECK')
          print('=' * 60)

          for asset in assets:
              asset_id = asset['id']
              asset_dir = static_dir / asset_id
              pre = pre_snapshot.get(asset_id, {'1h': 0, '1d': 0, '15m': 0, 'tweets': 0})

              required = ['tweet_events.json', 'prices_1h.json', 'prices_1d.json', 'stats.json']
              missing = [f for f in required if not (asset_dir / f).exists()]

              if missing:
                  errors.append(f'{asset_id}: missing {missing}')
                  print(f'‚ùå {asset_id.upper()}: missing {missing}')
                  continue

              # Get new counts
              new_counts = {}
              for tf in ['1h', '1d', '15m']:
                  f_path = asset_dir / f'prices_{tf}.json'
                  if f_path.exists():
                      with open(f_path) as f:
                          new_counts[tf] = json.load(f).get('count', 0)
                  else:
                      new_counts[tf] = 0

              with open(asset_dir / 'tweet_events.json') as f:
                  new_counts['tweets'] = json.load(f).get('count', 0)

              # Check for truncation (new count < 90% of old count)
              status = '‚úì'
              for key in ['1h', '1d', '15m', 'tweets']:
                  old_val = pre.get(key, 0)
                  new_val = new_counts.get(key, 0)
                  # Only check if there was previous data
                  if old_val > 10 and new_val < old_val * 0.9:
                      truncation_errors.append(
                          f'{asset_id}/{key}: {old_val} ‚Üí {new_val} (lost {old_val - new_val})'
                      )
                      status = '‚ùå TRUNCATED'

              print(f'{status} {asset_id.upper()}: 1h={new_counts["1h"]}, 1d={new_counts["1d"]}, tweets={new_counts["tweets"]}')
              if pre.get('1h', 0) > 0:
                  print(f'   (was: 1h={pre["1h"]}, 1d={pre["1d"]}, tweets={pre["tweets"]})')

          print()
          print('=' * 60)

          if truncation_errors:
              print('üö® DATA TRUNCATION DETECTED - BLOCKING COMMIT')
              print('The following data would be lost:')
              for e in truncation_errors:
                  print(f'   - {e}')
              print()
              print('This usually means the database is missing historical data.')
              print('To fix: restore DB from backup or re-fetch with --backfill')
              sys.exit(1)

          if errors:
              print(f'‚ö†Ô∏è  Validation warnings: {len(errors)}')
              for e in errors:
                  print(f'   - {e}')
          else:
              print('‚úÖ All assets validated - no truncation detected')
          PYTHON_SCRIPT

      - name: Generate timestamp
        run: |
          TIMESTAMP=$(date -u +%Y-%m-%dT%H:%M:%SZ)
          echo "{\"timestamp\": \"$TIMESTAMP\", \"source\": \"github-actions\"}" > web/public/static/last_updated.json
          echo "‚è∞ Timestamp: $TIMESTAMP"

      - name: Check for changes
        id: check-changes
        run: |
          git add -A
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "üìù Changes detected:"
            git diff --cached --stat | head -20
          fi

      - name: Commit and push changes
        if: steps.check-changes.outputs.changed == 'true'
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          TIMESTAMP=$(date -u +"%Y-%m-%d %H:%M UTC")
          git commit -m "chore: hourly data update $TIMESTAMP"
          git push
          echo "‚úÖ Changes pushed to repository"

      - name: Save database to cache
        uses: actions/cache/save@v4
        with:
          path: data/analytics.duckdb
          key: analytics-db-${{ github.run_id }}

      - name: Upload database to release (backup)
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          echo "üíæ Backing up database to release..."
          gh release upload database data/analytics.duckdb --clobber 2>/dev/null || \
            gh release create database data/analytics.duckdb --title "Database Backup" --notes "Auto-updated by hourly workflow"
          echo "‚úÖ Database backed up"

      - name: Summary
        run: |
          echo ""
          echo "========================================"
          echo "           WORKFLOW COMPLETE"
          echo "========================================"
          if [ "${{ steps.check-changes.outputs.changed }}" == "true" ]; then
            echo "‚úÖ Data updated and pushed"
            echo "üöÄ Vercel will auto-deploy shortly"
          else
            echo "‚ÑπÔ∏è  No new data to update"
          fi
          echo "========================================"
